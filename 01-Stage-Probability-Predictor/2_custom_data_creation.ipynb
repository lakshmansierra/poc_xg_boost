{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "37puETfgRzzg"
      },
      "source": [
        "# Custom Data Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EoRP98MpR-qj"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "N-qiINBQSK2g"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RopL7tUZSQkT"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pyspark_df = spark.table(\"sd_bdc_demo.sarima_time_series_forecasting.1_am_workday_updated_data\")\n",
        "# pyspark_df.display()\n",
        "# df = pyspark_df.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"1_am_workday_updated_data/1_am_workday_updated_data.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                Current_Role  Employee_ID  Years_Of_Service    Department  \\\n",
            "0   Senior Software Engineer           57               5.2   Engineering   \n",
            "1   Associate Data Scientist           58               1.7  Data Science   \n",
            "2  Associate Product Manager           59               1.3       Product   \n",
            "3           Business Analyst           60               3.9       Finance   \n",
            "4    Chief Operating Officer           61              15.2     Executive   \n",
            "\n",
            "   Employee_HR_rate  Hours_per_week  Fully_Loaded_Cost   Monthly_FLC  \\\n",
            "0                60              40          163355.20  13612.933333   \n",
            "1               117              40           92926.13   7743.844167   \n",
            "2               147              40          160081.53  13340.127500   \n",
            "3               153              40          105775.38   8814.615000   \n",
            "4                79              40          412283.65  34356.970833   \n",
            "\n",
            "   Years_Since_Last_Promotion  age  left  \n",
            "0                    2.814511   34     0  \n",
            "1                    1.577002   29     0  \n",
            "2                    1.259411   31     0  \n",
            "3                    2.310746   31     0  \n",
            "4                    5.796030   41     1  \n"
          ]
        }
      ],
      "source": [
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Current_Role', 'Employee_ID', 'Years_Of_Service', 'Department',\n",
            "       'Employee_HR_rate', 'Hours_per_week', 'Fully_Loaded_Cost',\n",
            "       'Monthly_FLC', 'Years_Since_Last_Promotion', 'age', 'left'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Time Series Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "# Prepare constants\n",
        "start_date = datetime.today().replace(day=1)\n",
        "months_back = 300\n",
        "date_range = [(start_date - relativedelta(months=i)).strftime(\"%Y-%m\") for i in range(months_back)][::-1]\n",
        "\n",
        "# Prepare the final data list\n",
        "final_data = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    emp_id = row[\"Employee_ID\"]\n",
        "    current_flc = row[\"Monthly_FLC\"]\n",
        "    if pd.isna(current_flc):\n",
        "        continue\n",
        "\n",
        "    # Calculate descending salary segments to simulate real hike breakage\n",
        "    segments = np.random.randint(3, 8)  # 3 to 7 segments (flat periods)\n",
        "    break_points = sorted(np.random.choice(range(1, months_back), size=segments - 1, replace=False))\n",
        "    break_points = [0] + break_points + [months_back]\n",
        "\n",
        "    # Generate values for each segment\n",
        "    segment_flcs = sorted(\n",
        "        np.round(np.random.normal(loc=current_flc * 0.7, scale=500, size=segments)), reverse=False\n",
        "    )\n",
        "    segment_flcs[-1] = current_flc  # Set last segment to current average\n",
        "\n",
        "    monthly_costs = []\n",
        "    for i in range(len(break_points) - 1):\n",
        "        monthly_costs.extend([segment_flcs[i]] * (break_points[i + 1] - break_points[i]))\n",
        "\n",
        "    for ym, cost in zip(date_range, monthly_costs):\n",
        "        final_data.append({\n",
        "            \"Employee_ID\": emp_id,\n",
        "            \"year_month\": ym,\n",
        "            \"Annual_load_cost_per_month\": round(cost, 2)\n",
        "        })\n",
        "\n",
        "# Convert to DataFrame\n",
        "flc_time_series_df = pd.DataFrame(final_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export Updated Excel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the folder if it doesn't exist\n",
        "os.makedirs(\"2_am_workday_historic_time_series_updated_data\", exist_ok=True)\n",
        "\n",
        "# Save the Excel file inside the folder\n",
        "flc_time_series_df.to_csv(\"2_am_workday_historic_time_series_updated_data/2_am_workday_historic_time_series_updated_data.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# spark_df = spark.createDataFrame(flc_time_series_df)\n",
        "# spark_df.write.mode(\"overwrite\").saveAsTable(\"sd_bdc_demo.sarima_time_series_forecasting.2_am_workday_historic_time_series_updated_data\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "data_preprocessing_tools.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
